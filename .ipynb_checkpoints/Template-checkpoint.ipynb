{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing general libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# data analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# metrics and algorithm validation\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score\n",
    "from sklearn.model_selection import KFold, cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import the dataset\n",
    "df = pd.read_csv('Social_Network_Ads.csv')\n",
    "X = df.iloc[:,1:-1].values\n",
    "y = df.iloc[:,-1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill empty rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# filling empty rows with the mean\n",
    "# doing it by the column, select only numerical columns\n",
    "from sklearn.preprocessing import Imputer\n",
    "imputer = Imputer()\n",
    "imputer = imputer.fit_transform(X[:,'columns':])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# encoding categorical data\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "X[:,0] = encoder.fit_transform(X[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create dummy variables\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "hot_encoder = OneHotEncoder(categorical_features=[0])\n",
    "X = hot_encoder.fit_transform(X).toarray()\n",
    "X = X[:, 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backward elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as sm\n",
    "# a column of ones needs to be added so that the formula will work as intended\n",
    "X = np.append(values=X, arr=np.ones((50,1)).astype(int),axis=1)\n",
    "\n",
    "# repeat the 3 commands underneath and eliminate one row at each stept until all are within the desired p statistic\n",
    "X_opt = X[:, [0,1,2,3,4,5,\"\"\"all X values\"\"\"]]\n",
    "# ordinary least squares regressor\n",
    "regressor_OLS = sm.OLS(y,X_opt).fit()\n",
    "regressor_OLS.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing for algorithm application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# splitting the train and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# feature scaling\n",
    "# required for K-NN, K-means, logistic regression, SVMs, perceptrons, neural networks, LDA, PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_X = StandardScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "# leave it as None initially to explore the variance first, then change to the choosen number from explained_variance\n",
    "pca = PCA(n_components=None)\n",
    "\n",
    "# fitting and transforming the training set and transforming the test set\n",
    "X_train = pca.fit_transform(X_train)\n",
    "X_test = pca.transform(X_test)\n",
    "\n",
    "# cumulated explained vairance of the principal components\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "explained_variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kernel PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "kpca = KernelPCA(n_components=None,kernel='rbf')\n",
    "\n",
    "# fitting and transforming the training set and transforming the test set\n",
    "X_train = kpca.fit_transform(X_train)\n",
    "X_test = kpca.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# applying LDA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "lda = LinearDiscriminantAnalysis(n_components=2)\n",
    "\n",
    "# fitting and transforming the training set and transforming the test set\n",
    "X_train = lda.fit_transform(X_train,y_train)\n",
    "X_test = lda.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "from sklearn.linear_model import LinearRegression as LR\n",
    "from sklearn.preprocessing import PolynomialFeatures as Poly\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor as RT\n",
    "from sklearn.ensemble import RandomForestRegressor as RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cross validation\n",
    "\n",
    "algorithms = []\n",
    "\n",
    "algorithms.append(('LR', LR()))\n",
    "algorithms.append(('Poly', Poly(degree='select')))\n",
    "algorithms.append(('SVR', SVR(kernel='select')))\n",
    "algorithms.append(('CART', RT('hyperparams')))\n",
    "algorithms.append(('RF', RF('hyperparams')))\n",
    "\n",
    "results = []\n",
    "names = []\n",
    "scoring = 'r2'\n",
    "\n",
    "for name, model in algorithms:\n",
    "    cv_results = cross_val_score(model, X_train, y_train, cv=10, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = '%s: %f (%f)' %(name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)\n",
    "    \n",
    "fig = plt.figure(figsize=(22,5))\n",
    "fig.suptitle('Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "from sklearn.linear_model import LogisticRegression as LGR\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB as NB\n",
    "from sklearn.tree import DecisionTreeClassifier as CT\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "from sklearn.ensemble import AdaBoostClassifier as ADA\n",
    "from sklearn.ensemble import GradientBoostingClassifier as GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# cross validation\n",
    "\n",
    "algorithms = []\n",
    "\n",
    "algorithms.append(('LGR', LGR()))\n",
    "algorithms.append(('KNN', KNN()))\n",
    "algorithms.append(('SVC', SVC(kernel='rbf')))\n",
    "algorithms.append(('NB', NB()))\n",
    "algorithms.append(('CART', CT()))\n",
    "algorithms.append(('RFC', RFC()))\n",
    "algorithms.append(('ADA', ADA()))\n",
    "algorithms.append(('GB', GB()))\n",
    "\n",
    "results = []\n",
    "names = []\n",
    "scoring = 'accuracy'\n",
    "\n",
    "for name, model in algorithms:\n",
    "    cv_results = cross_val_score(model, X_train, y_train, cv=10, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = '%s: %f (%f)' %(name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)\n",
    "    \n",
    "fig = plt.figure(figsize=(22,10))\n",
    "fig.suptitle('Algorithm Comparison',size='xx-large')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names,size='xx-large')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# finding the best model and the best hyperparams\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# specifying the combinations of different params of which we want to find the optimal values\n",
    "# use this to find out whether the problem si linear or non-linear\n",
    "# those will be applied to a particular model - here those are for SVC to check for linear or no and other hyperparam tunning\n",
    "params = [{'C': [0.01,0.03,0.1,0.3,1,10,100,1000],\n",
    "           'kernel': ['linear']},\n",
    "          {'C': [0.01,0.03,0.1,0.3,1,10,100,1000],\n",
    "           'kernel': ['rbf'],\n",
    "           'gamma': [0.001,0.01,0.1,0.3,0.5,0.7,1]},\n",
    "         ]\n",
    "\n",
    "# putting it all together\n",
    "# n_jobs = -1 for large datasets\n",
    "# classifier = the model allocated to the variable\n",
    "grid_search = GridSearchCV(estimator=classifier, param_grid=params, scoring='accuracy', cv=10, n_jobs=-1)\n",
    "\n",
    "# fitting\n",
    "grid_search = grid_search.fit(X_train,y_train)\n",
    "\n",
    "# best params\n",
    "# after finding out the best params, you can reassess and try new params\n",
    "print('Best params: ', grid_search.best_params_)\n",
    "\n",
    "# best accuracy - mean of the 10 accuracies through 10 CV combinations\n",
    "print('Best accuracy: ', grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualise results (2D)\n",
    "\n",
    "Use PCA or LDA to convert to 2D before running this if the data is not in 2D already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "X_set, y_set = X_train, y_train\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "X1, X2 = np.meshgrid(np.arange(start = X_set[:,0].min() - 1, stop = X_set[:,0].max() + 1, step = 0.01),\n",
    "                     np.arange(start = X_set[:,1].min() - 1, stop = X_set[:,1].max() + 1, step = 0.01))\n",
    "\n",
    "plt.contourf(X1,X2,classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n",
    "            alpha = 0.75, cmap = ListedColormap(('red','green')))\n",
    "\n",
    "plt.xlim(X1.min(), X1.max())\n",
    "plt.ylim(X2.min(), X2.max())\n",
    "\n",
    "for i,j in enumerate(np.unique(y_set)):\n",
    "    plt.scatter(X_set[y_set == j,0], X_set[y_set == j, 1],\n",
    "               c = ListedColormap(('red','green'))(i), label = j)\n",
    "\n",
    "plt.title('Title')\n",
    "plt.xlabel('Variable 1')\n",
    "plt.ylabel('Variable 2')\n",
    "plt.legend(bbox_to_anchor=(1.05,1))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
